{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "from nltk import ngrams\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>frequency</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>275</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31</td>\n",
              "      <td>a</td>\n",
              "      <td>aaa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>a</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45</td>\n",
              "      <td>a</td>\n",
              "      <td>an</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>192</td>\n",
              "      <td>a</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   frequency  0    1\n",
              "0        275  a    a\n",
              "1         31  a  aaa\n",
              "2         29  a  all\n",
              "3         45  a   an\n",
              "4        192  a  and"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "bigrams = pd.read_csv(\"w2_.txt\", sep=\"\\t\", header=None, encoding='latin-1')\n",
        "bigrams.columns = [\"frequency\", \"0\", \"1\"]\n",
        "bigrams = bigrams.dropna()\n",
        "bigrams.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>frequency</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16</td>\n",
              "      <td>a</td>\n",
              "      <td>babe</td>\n",
              "      <td>in</td>\n",
              "      <td>the</td>\n",
              "      <td>woods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>a</td>\n",
              "      <td>baby</td>\n",
              "      <td>at</td>\n",
              "      <td>her</td>\n",
              "      <td>breast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>a</td>\n",
              "      <td>baby</td>\n",
              "      <td>brother</td>\n",
              "      <td>or</td>\n",
              "      <td>sister</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>a</td>\n",
              "      <td>baby</td>\n",
              "      <td>crying</td>\n",
              "      <td>in</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>a</td>\n",
              "      <td>baby</td>\n",
              "      <td>girl</td>\n",
              "      <td>was</td>\n",
              "      <td>born</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   frequency  0     1        2    3       4\n",
              "0         16  a  babe       in  the   woods\n",
              "1          6  a  baby       at  her  breast\n",
              "2          9  a  baby  brother   or  sister\n",
              "3          6  a  baby   crying   in     the\n",
              "4          6  a  baby     girl  was    born"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "fivegrams = pd.read_csv(\"fivegrams.txt\", sep=\"\\t\", header=None)\n",
        "fivegrams.columns = [\"frequency\", \"0\", \"1\", \"2\", \"3\", \"4\"]\n",
        "fivegrams.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "\n",
        "\n",
        "class SpellCorrector:\n",
        "    def __init__(self, grams: pd.DataFrame, n_grams: int, word_threshold: int = 50) -> None:\n",
        "        self.n_grams = n_grams\n",
        "        self.word_freq = Counter(grams[\"0\"])\n",
        "        for i in range(1, n_grams):\n",
        "            self.word_freq.update(grams[str(i)])\n",
        "            \n",
        "        _to_drop = []\n",
        "        for word, freq in self.word_freq.items():\n",
        "            if freq < word_threshold:\n",
        "                _to_drop.append(word)\n",
        "        \n",
        "        _stay_index = ~grams['0'].isin(_to_drop)\n",
        "        for i in range(1, self.n_grams):\n",
        "            _stay_index = _stay_index & (grams[str(i)].isin(_to_drop))\n",
        "            \n",
        "        self.grams = grams[_stay_index]\n",
        "        \n",
        "        self.word_freq = Counter({k: c for k, c in self.word_freq.items() if c >= word_threshold})\n",
        "        self.ALL_WORDS = set(self.word_freq)\n",
        "        \n",
        "    \n",
        "    def _get_candidates(self, word: str) -> tuple[list[str], list[str]]:\n",
        "        dist1candidates = edits1(word)\n",
        "        dist2candidates = set(e2 for e1 in dist1candidates for e2 in edits1(e1))\n",
        "        dist2candidates = dist2candidates - dist1candidates\n",
        "\n",
        "        dist1candidates = list(filter(lambda x: x in self.ALL_WORDS, dist1candidates))\n",
        "        dist2candidates = list(filter(lambda x: x in self.ALL_WORDS, dist2candidates))\n",
        "        return dist1candidates, dist2candidates\n",
        "        \n",
        "    def _one_word_case(self, word: str) -> str:\n",
        "        first_candidates, second_candidates = self._get_candidates(word)\n",
        "        word2prob = []\n",
        "        for candidate in first_candidates:\n",
        "            candidate_prob = self.word_freq.get(candidate) / self.word_freq.total()\n",
        "            word2prob.append((candidate, candidate_prob))\n",
        "        \n",
        "        for candidate in second_candidates:\n",
        "            candidate_prob = self.word_freq.get(candidate) / self.word_freq.total()\n",
        "            candidate_prob *= 1/2\n",
        "            # assumption that if 2 mistakes is 2 time worse\n",
        "            word2prob.append((candidate, candidate_prob))\n",
        "        \n",
        "        if len(word2prob) == 0:\n",
        "            return word\n",
        "    \n",
        "        corrected_word, prob = max(word2prob, key=lambda x: x[1])\n",
        "        return corrected_word\n",
        "\n",
        "    def correct(self, text: str) -> str:\n",
        "        text = text.lower()\n",
        "        words = text.split()\n",
        "        \n",
        "        corrected = []\n",
        "        \n",
        "        for i in range(min(len(words), self.n_grams-1)):\n",
        "            word = words[i]\n",
        "            if word in self.ALL_WORDS:\n",
        "                # for now if word exists we will not change it\n",
        "                corrected.append(word)\n",
        "                continue\n",
        "            corrected_word = self._one_word_case(word)\n",
        "            corrected.append(corrected_word)\n",
        "        \n",
        "        for i in range(self.n_grams-1, len(words)):\n",
        "            word = words[i]\n",
        "            if word in self.ALL_WORDS:\n",
        "                corrected.append(word)\n",
        "                continue\n",
        "                \n",
        "            context = self.grams[self.grams[\"0\"] == corrected[-self.n_grams + 1]]\n",
        "            for j in range(1, self.n_grams-1):\n",
        "                context = context[context[str(j)] == corrected[j - self.n_grams + 1]]\n",
        "            \n",
        "            if context.shape[0] == 0:\n",
        "                corrected.append(self._one_word_case(word))\n",
        "                continue\n",
        "            \n",
        "            first_candidates, second_candidates = self._get_candidates(word)\n",
        "            word2prob = []\n",
        "            for candidate in first_candidates:\n",
        "                tmp = context[context[str(self.n_grams-1)] == candidate]\n",
        "                if tmp.shape[0] == 0:\n",
        "                    word2prob.append((candidate, 0.0))\n",
        "                else:\n",
        "                    tmp = tmp['frequency'].iloc[0]\n",
        "                    prob = tmp / context[\"frequency\"].sum()\n",
        "                    word2prob.append((candidate, prob))\n",
        "            \n",
        "            for candidate in second_candidates:\n",
        "                tmp = context[context[str(self.n_grams-1)] == candidate]\n",
        "                if tmp.shape[0] == 0:\n",
        "                    word2prob.append((candidate, 0.0))\n",
        "                else:\n",
        "                    tmp = tmp['frequency'].iloc[0]\n",
        "                    prob = tmp / context[\"frequency\"].sum()\n",
        "                    prob /= 2\n",
        "                    word2prob.append((candidate, prob))\n",
        "            if len(word2prob) == 0:\n",
        "                corrected.append(word)\n",
        "                continue\n",
        "            corrected_word, prob = max(word2prob, key=lambda x: x[1])\n",
        "            if prob > 0.0:\n",
        "                corrected.append(corrected_word)\n",
        "            else:\n",
        "                corrected.append(word)\n",
        "        \n",
        "        return \" \".join(corrected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i mde a hge correct'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corrector = SpellCorrector(bigrams, 2)\n",
        "corrector.correct(\"i mde a hge corrector\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i made a huge collection'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from jellyfish import damerau_levenshtein_distance\n",
        "\n",
        "class BigramPredictonCorrector:\n",
        "    def __init__(self, bigrams: pd.DataFrame, word_threshold=50) -> None:\n",
        "        self.word_freq = Counter(bigrams[\"0\"])\n",
        "        self.word_freq.update(bigrams[\"1\"])\n",
        "        self.word_threshold = word_threshold\n",
        "        \n",
        "        _to_drop = []\n",
        "        for word, freq in self.word_freq.items():\n",
        "            if freq < word_threshold:\n",
        "                _to_drop.append(word)\n",
        "        \n",
        "        self.word_freq = Counter({k: c for k, c in self.word_freq.items() if c >= word_threshold})\n",
        "        \n",
        "        bigrams = bigrams[(~bigrams['1'].isin(_to_drop)) & (~bigrams['0'].isin(_to_drop))]\n",
        "            \n",
        "        self.ALL_WORDS = set(self.word_freq)\n",
        "        self.bigrams = bigrams\n",
        "    \n",
        "    def correct(self, text: str):\n",
        "        text = text.lower()\n",
        "        tokens = text.split()\n",
        "        \n",
        "        # initally 'a' token will be there\n",
        "        corrected = [\"a\"]\n",
        "        \n",
        "        for word in tokens:\n",
        "            if word in self.ALL_WORDS:\n",
        "                # print(\"IN DICTIONARY:\", word)\n",
        "                corrected.append(word)\n",
        "                continue\n",
        "            \n",
        "            candidates: pd.DataFrame = self.bigrams[self.bigrams['0'] == corrected[-1]]\n",
        "            candidates = candidates[['frequency', '1']]\n",
        "            context_support = int(candidates['frequency'].sum())\n",
        "            \n",
        "            best_candidate = word\n",
        "            best_prob = -1\n",
        "            best_dist = len(word) * 2\n",
        "            for _, (freq, candidate) in candidates.iterrows():\n",
        "                dist = damerau_levenshtein_distance(candidate, word)\n",
        "                prob = freq / context_support\n",
        "                prob *= 1/(dist+1)\n",
        "                \n",
        "                if (dist < best_dist) or (dist == best_dist and prob > best_prob):\n",
        "                    best_prob = prob\n",
        "                    best_dist = dist\n",
        "                    best_candidate = candidate \n",
        "                    \n",
        "            corrected.append(best_candidate)\n",
        "            \n",
        "        return \" \".join(corrected[1:])\n",
        "                \n",
        "\n",
        "corrector = BigramPredictonCorrector(bigrams)\n",
        "corrector.correct(\"i mde a hge corrector\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Norvig solution for further tests and comparison\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "# WORDS = Counter(words(open('big.txt').read()))\n",
        "WORDS = Counter(corrector.ALL_WORDS)\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "# Justifications\n",
        "\n",
        "I decided to implement two approaches:\n",
        "- Nordvig based approach that will compute word probability using N-gramm model (`SpellCorrector`)\n",
        "- N-gramm candidate approach. That will use previous words and their possible next word as candidates. And also, it will use N-gramm model for predictin word probability (`BigramPredictonCorrector`)\n",
        "\n",
        "## SpellCorector\n",
        "\n",
        "### Selection mechanism\n",
        "\n",
        "Simple argmax\n",
        "\n",
        "### Candidate model\n",
        "\n",
        "I used same candidate model as Nordvig solution. Build edit distance of 1 and 2 of word. And check if they are in dictionary of existing words\n",
        "\n",
        "### Language model\n",
        "\n",
        "As language model I use bigram model, because 3-grams or more are to specific for simple typo correction and most importantly there are more data in bigrams.\n",
        "\n",
        "I count frequency of previous word and candidate and divide by all bigrams with same previois word.\n",
        "\n",
        "### Error model\n",
        "\n",
        "As error model I use following:\n",
        "- if edit distance is 1, probability is 1\n",
        "- if edit distance is 2, probability is 0.5\n",
        "\n",
        "It is simple and dummy approach. Probabilities here are not normalised due to argmax nature. Intuition behind: every mistake should divide our confidence in half.\n",
        "\n",
        "## BigramPredictonCorrector\n",
        "\n",
        "### Selection mechanism\n",
        "\n",
        "Simple argmax\n",
        "\n",
        "### Candidate model\n",
        "\n",
        "Previous word in text is analyzed. And all possible continuation in bigrams dictionary are our candidates. For example, for pair `good fsh` candidate `x` will have following entry in bigrams dictionary `good x`.\n",
        "\n",
        "### Language model\n",
        "\n",
        "Here n-gram model is also used. Probability computed as frequency of pair `(prefix, candidate)` divided by sum of all frequencies `(prefix, x)`, where x are possible candidates\n",
        "\n",
        "### Error model\n",
        "\n",
        "For error model i used Damerau Levenshtein distance which adds swapping of characters with cost 1. I consider that less edit distance will be infitely more likely. So in the end least edit distance is chosen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test set generation is done by following operations:\n",
        "- adding random character\n",
        "- swapping 2 neighbour characters\n",
        "- duplicate 1 character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mein leben gst gut und schon drj hnud ist klein aaber schou'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "def introduce_errors(words: list[str], error_rate:float=0.1) -> str:\n",
        "    num_errors = int(len(words) * error_rate)\n",
        "    \n",
        "    for _ in range(num_errors):\n",
        "        index = random.randint(0, len(words) - 1)\n",
        "        word = words[index]\n",
        "        if len(word) > 1:\n",
        "            choice = random.randint(0, 2)\n",
        "            if choice == 0:\n",
        "                # swap\n",
        "                error_index = random.randint(0, len(word) - 2)\n",
        "                word = list(word)\n",
        "                word[error_index], word[error_index + 1] = word[error_index + 1], word[error_index]\n",
        "                words[index] = ''.join(word)\n",
        "            elif choice == 1:\n",
        "                # duplicate\n",
        "                error_index = random.randint(0, len(word) - 1)\n",
        "                word = word[:error_index] + word[error_index] + word[error_index:]\n",
        "                words[index] = word\n",
        "            else:\n",
        "                # random addition\n",
        "                error_index = random.randint(0, len(word) - 1)\n",
        "                typo = chr(ord('a') + random.randint(0, 25))\n",
        "                words[index] = word[:error_index] + typo + word[error_index + 1:]\n",
        "    \n",
        "    \n",
        "    return ' '.join(words)\n",
        "introduce_errors(\"mein leben ist gut und schon der hund ist klein aber schon\".split(), error_rate=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_test_set(noise=0.1):\n",
        "    with open(\"big.txt\") as f:\n",
        "        big = f.readlines()\n",
        "\n",
        "    big = filter(lambda x: len(x) > 2, map(lambda x: x.replace('\\n', ''), big))\n",
        "    x = []\n",
        "    y = []\n",
        "    for line in big:\n",
        "        line = line.lower()\n",
        "        words = re.findall(r'\\w+', line)\n",
        "        x.append(\" \".join(words))\n",
        "        text = introduce_errors(words, error_rate=noise)\n",
        "        y.append(text)\n",
        "    return x, y\n",
        "\n",
        "x, y = get_test_set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the project gutenberg ebook of the adventures of sherlock holmes\n",
            "the project gutenberg ebook of the adventures of sherlock homles\n",
            "--------------------\n",
            "by sir arthur conan doyle\n",
            "by sir arthur conan doyle\n",
            "--------------------\n",
            "15 in our series by sir arthur conan doyle\n",
            "15 in our series by sir arthur conan doyle\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    print(x[i])\n",
        "    print(y[i])\n",
        "    print(\"-\"*20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1035/1035 [06:13<00:00,  2.77it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import trange, tqdm\n",
        "\n",
        "def test(x, y, sample=1):\n",
        "    corrector1 = SpellCorrector(bigrams, 2)\n",
        "    corrector2 = BigramPredictonCorrector(bigrams)\n",
        "    \n",
        "    n_bad1, n_bad2, n_bad3 = 0, 0, 0\n",
        "    total = 0\n",
        "    \n",
        "    sample_indexes = [i for i in range(len(x))]\n",
        "    random.shuffle(sample_indexes)\n",
        "    sample_indexes = sample_indexes[:round(sample * len(x))]\n",
        "    for i in tqdm(sample_indexes):\n",
        "        text = x[i]\n",
        "        target = y[i].split()\n",
        "        total += len(target)\n",
        "        out1 = corrector1.correct(text).split()\n",
        "        out2 = corrector2.correct(text).split()\n",
        "        out3 = list(map(correction, text.split()))\n",
        "\n",
        "        for j in range(len(target)):\n",
        "            if target[j] != out1[j]:\n",
        "                n_bad1 += 1\n",
        "            if target[j] != out2[j]:\n",
        "                n_bad2 += 1\n",
        "            if target[j] != out3[j]:\n",
        "                n_bad3 += 1\n",
        "    return 1 - n_bad1/total, 1 - n_bad2/total, 1 - n_bad3/total\n",
        "\n",
        "acc1, acc2, norvig = test(x, y, sample=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig solution accuracy: 0.8300966569122994\n",
            "Norvig solution with bigram model accuracy: 0.8989979604504744\n",
            "Bigram prediction model accuracy 0.771836481333688\n"
          ]
        }
      ],
      "source": [
        "print(f\"Norvig solution accuracy: {norvig}\")\n",
        "print(f\"Norvig solution with bigram model accuracy: {acc1}\")\n",
        "print(f\"Bigram prediction model accuracy {acc2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see that Norvig solution with Bigram model produced best result. And bigram prediction model has lowest probability. Maybe, reason that edit distance if key factor. However, if I trie to use some multiplication of edit distance and candidate probability, I will get most popular word that is not close to original."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
